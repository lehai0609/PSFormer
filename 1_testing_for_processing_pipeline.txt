This plan is specifically designed to identify failures based on the model's architecture as described in the research paper, especially concerning how data is prepared for the encoder and its internal **Parameter Shared Blocks**.

### **Test Suite Plan: `TestInputProcessingPipeline`**

**Test Fixture Setup:**
A common setup for all tests that initializes a default `PSformer` model instance with a standard configuration (e.g., `sequence_length=512`, `patch_size=32`, `num_variables=7`, `num_encoder_layers=3`).

```pseudocode
SETUP
    config = {
        sequence_length: 512,
        patch_size: 32,
        num_variables: 7,
        num_encoder_layers: 3
    }
    psformer_model = create_psformer(config)
    
    // Derived values for assertions
    batch_size = 4
    N_segments = 512 / 32 = 16
    C_segment_length = 7 * 32 = 224
END SETUP
```

---

### **1. Data Validation Tests**

*Purpose: To ensure the pipeline fails loudly and predictably with malformed raw input.*

**Test Case 1.1: `test_pipeline_rejects_input_with_incorrect_dimensions`**
*Motivation: The entire patching and segmenting logic relies on a 3D tensor `[Batch, M, L]`.*

```pseudocode
TEST test_pipeline_rejects_input_with_incorrect_dimensions
    // SETUP
    input_2d = create_tensor(shape=(batch_size, config.sequence_length))
    input_4d = create_tensor(shape=(batch_size, config.num_variables, config.sequence_length, 1))

    // ACTION & ASSERT
    ASSERT pipeline raises ValueError when called with input_2d
    ASSERT pipeline raises ValueError when called with input_4d
END TEST
```

**Test Case 1.2: `test_pipeline_rejects_mismatched_sequence_length`**
*Motivation: The patching logic (`L = N * P`) requires the sequence length `L` to match the model's configuration.*

```pseudocode
TEST test_pipeline_rejects_mismatched_sequence_length
    // SETUP
    wrong_length_tensor = create_tensor(shape=(batch_size, config.num_variables, 100)) // Expected 512

    // ACTION & ASSERT
    // This failure should originate from the PSformerDataTransformer
    ASSERT pipeline raises ValueError when called with wrong_length_tensor
END TEST
```

---

### **2. Processing/Feature Engineering Tests**

*Purpose: To validate the intermediate normalization and transformation steps.*

**Test Case 2.1: `test_revin_normalizes_data_per_channel`**
*Motivation: As per the paper (Section 3.2), RevIN is the first step. We must verify its effect. The statistics should be computed per variable (channel), not globally.*

```pseudocode
TEST test_revin_normalizes_data_per_channel
    // SETUP
    // Create an input where each channel has a distinct, non-zero mean and std dev
    raw_input = create_tensor(shape=(batch_size, config.num_variables, config.sequence_length))
    raw_input[0, 0, :] = raw_input[0, 0, :] + 10 // Shift mean of first channel
    raw_input[0, 1, :] = raw_input[0, 1, :] * 5   // Scale variance of second channel
    
    // ACTION
    // Use a 'hook' to capture the input to the data_transformer, which is the output of RevIN
    capture_revin_output = register_hook(psformer_model.data_transformer)
    call_forward_pass(psformer_model, raw_input)
    normalized_data = capture_revin_output.get_captured_tensor()

    // ASSERT
    // Check mean and std dev for a specific, modified channel
    ASSERT mean of normalized_data[0, 0, :] is approximately 0.0
    ASSERT std_dev of normalized_data[0, 0, :] is approximately 1.0
    
    // Verify it didn't wrongly normalize other channels
    ASSERT mean of normalized_data[0, 2, :] is also approximately 0.0
END TEST
```

**Test Case 2.2: `test_revin_stores_statistics_for_denormalization`**
*Motivation: The RevIN layer must retain the mean/stdev to perform the inverse operation on the model's final output. This test ensures the state is correctly stored.*

```pseudocode
TEST test_revin_stores_statistics_for_denormalization
    // SETUP
    raw_input = create_tensor(shape=(batch_size, config.num_variables, config.sequence_length))
    
    // ACTION
    call_forward_pass(psformer_model, raw_input)

    // ASSERT
    ASSERT psformer_model.revin_layer.mean is not None
    ASSERT psformer_model.revin_layer.stdev is not None
    // Statistics should be stored per-channel, with a shape ready for broadcasting
    ASSERT shape of psformer_model.revin_layer.mean is (batch_size, config.num_variables, 1)
END TEST
```

---

### **3. Model Behavior Tests**

*Purpose: To assert the final output shape of the pipeline (the encoder's output).*

**Test Case 3.1: `test_pipeline_output_shape_is_correct`**
*Motivation: The most critical test. This verifies that the entire chain of transformations (`RevIN` -> `Patching` -> `Segmenting` -> `Encoder`) produces an output with the expected shape `[B, N, C]`.*

```pseudocode
TEST test_pipeline_output_shape_is_correct
    // SETUP
    raw_input = create_tensor(shape=(batch_size, config.num_variables, config.sequence_length))
    
    // ACTION
    encoder_output, attention_weights = call_forward_pass(psformer_model, raw_input)
    
    // ASSERT
    // Output of encoder should have same shape as its input: [Batch, Num_Segments, Segment_Length]
    ASSERT shape of encoder_output is (batch_size, N_segments, C_segment_length)
END TEST
```

**Test Case 3.2: `test_attention_weights_have_correct_shape_and_structure`**
*Motivation: The pipeline's output includes attention weights, which are crucial for model interpretability. Their shape confirms the attention mechanism is working as described in the paper (attention over dimension `C`).*

```pseudocode
TEST test_attention_weights_have_correct_shape_and_structure
    // SETUP
    raw_input = create_tensor(shape=(batch_size, config.num_variables, config.sequence_length))

    // ACTION
    encoder_output, attention_weights_list = call_forward_pass(psformer_model, raw_input)

    // ASSERT
    // The list should contain one set of weights per encoder layer
    ASSERT length of attention_weights_list is config.num_encoder_layers

    // Each element is a tuple for (stage1_weights, stage2_weights)
    stage1_weights, stage2_weights = attention_weights_list[0]

    // As per Fig. 2 & 4, attention is on segments of length C.
    // So the attention map should be [Batch, C, C]
    ASSERT shape of stage1_weights is (batch_size, C_segment_length, C_segment_length)
    ASSERT shape of stage2_weights is (batch_size, C_segment_length, C_segment_length)
END TEST
```

---

### **4. Performance and Robustness Tests**

*Purpose: To simulate adverse data conditions.*

**Test Case 4.1: `test_pipeline_handles_input_with_nan_or_inf`**
*Motivation: Real-world data is messy. The model shouldn't crash on bad data.*

```pseudocode
TEST test_pipeline_handles_input_with_nan_or_inf
    // SETUP
    input_with_nan = create_tensor(...)
    input_with_nan[0, 0, 0] = NaN
    
    // ACTION
    encoder_output, _ = call_forward_pass(psformer_model, input_with_nan)
    
    // ASSERT
    // The model should not crash. The NaN should propagate.
    ASSERT output contains NaN
END TEST
```

**Test Case 4.2: `test_pipeline_handles_zero_variance_channel`**
*Motivation: A channel with no variance (a flat line) can cause division-by-zero in normalization. The `eps` in RevIN is designed to prevent this.*

```pseudocode
TEST test_pipeline_handles_zero_variance_channel
    // SETUP
    input_zero_var = create_tensor_of_ones(shape=(batch_size, config.num_variables, config.sequence_length))

    // ACTION
    encoder_output, _ = call_forward_pass(psformer_model, input_zero_var)

    // ASSERT
    // The output should be all finite numbers, not NaN or Inf.
    ASSERT output is all finite
END TEST
```

---

### **5. Architecture Tests**

*Purpose: To ensure the components are wired together correctly, as per the paper's design.*

**Test Case 5.1: `test_encoder_ps_block_dimension_matches_data_transformer_segment_length`**
*Motivation: This is the **most important architecture test**. It directly connects the **Parameter Shared Block** design to the input pipeline. The `PSBlock` (Figure 2) has internal `N x N` matrices. In your code, this corresponds to the `PSBlock(N=...)` parameter. For the `PSformerEncoder`, this `N` must be equal to the feature dimension `C` (segment length) produced by the `PSformerDataTransformer`. This test verifies that the data is prepared in the exact format the core building block expects.*

```pseudocode
TEST test_encoder_ps_block_dimension_matches_data_transformer_segment_length
    // SETUP
    // Model is already created in the fixture.
    
    // ACTION
    // Get the segment length C from the data transformer's config
    C_from_transformer = psformer_model.data_transformer.config.segment_length
    
    // Get the internal dimension N from the first PS Block inside the first encoder layer
    // This requires inspecting the model's internal structure.
    N_from_ps_block = psformer_model.encoder.layers[0].ps_block.N

    // ASSERT
    ASSERT C_from_transformer equals N_from_ps_block
    ASSERT N_from_ps_block equals C_segment_length // C = M * P
END TEST
```