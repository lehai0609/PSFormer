{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PSformer for Stock Forecasting\n",
    "\n",
    "This notebook implements the PSformer (Parameter Shared Transformer) model for stock price forecasting using multiple tickers.\n",
    "\n",
    "**Important Disclaimer:** This model uses random weights and serves as a demonstration. For real-world applications, the model needs to be properly trained on historical data.\n",
    "\n",
    "## Features:\n",
    "- Multi-ticker stock forecasting\n",
    "- Backtest validation on held-out data\n",
    "- Parameter sharing for efficient computation\n",
    "- RevIN normalization for better generalization\n",
    "- Two-stage segment attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch pandas numpy matplotlib plotly\n",
    "\n",
    "# Mount Google Drive (optional)\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from typing import Tuple, Optional, Dict, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Setup complete! PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload Your Data Files\n",
    "\n",
    "**Instructions:**\n",
    "1. Use the file explorer pane on the left side of Colab\n",
    "2. Upload your OHLCV data file (CSV format)\n",
    "3. The CSV should have columns: ['Date', 'Ticker', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
    "4. Make sure the Date column is properly formatted\n",
    "\n",
    "**Expected CSV format:**\n",
    "```\n",
    "Date,Ticker,Open,High,Low,Close,Volume\n",
    "2023-01-01,AAPL,150.0,155.0,149.0,154.0,1000000\n",
    "2023-01-02,AAPL,154.0,156.0,152.0,155.0,1100000\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List files in current directory to verify upload\n",
    "import os\n",
    "print(\"Files in current directory:\")\n",
    "for file in os.listdir('.'):\n",
    "    if file.endswith('.csv'):\n",
    "        print(f\"üìä {file}\")\n",
    "    else:\n",
    "        print(f\"üìÑ {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "\n",
    "This cell contains all the parameters that can be easily modified for different experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== DATA CONFIGURATION ==========\n",
    "DATA_FILE_PATH = \"stock_data.csv\"  # Change this to your uploaded CSV file name\n",
    "TICKER_COLUMN = \"Ticker\"\n",
    "DATE_COLUMN = \"Date\"\n",
    "OHLCV_COLUMNS = [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]\n",
    "\n",
    "# ========== MODEL HYPERPARAMETERS ==========\n",
    "SEQUENCE_LENGTH = 96    # Input sequence length (L) - 96 days of historical data\n",
    "PATCH_SIZE = 8          # Temporal patch size (P) - 8 days per patch\n",
    "PREDICTION_LENGTH = 30  # Forecast horizon (F) - 30 days ahead\n",
    "NUM_ENCODER_LAYERS = 2  # Number of PSformer encoder layers\n",
    "NUM_VARIABLES = 5       # Number of features (Open, High, Low, Close, Volume)\n",
    "\n",
    "# ========== VALIDATION CONFIGURATION ==========\n",
    "MIN_DATA_POINTS = SEQUENCE_LENGTH + PREDICTION_LENGTH  # 126 days minimum\n",
    "\n",
    "# ========== DEVICE CONFIGURATION ==========\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Verify configuration\n",
    "print(f\"\\nConfiguration Summary:\")\n",
    "print(f\"- Input sequence length: {SEQUENCE_LENGTH} days\")\n",
    "print(f\"- Patch size: {PATCH_SIZE} days\")\n",
    "print(f\"- Number of patches: {SEQUENCE_LENGTH // PATCH_SIZE}\")\n",
    "print(f\"- Prediction horizon: {PREDICTION_LENGTH} days\")\n",
    "print(f\"- Minimum data required: {MIN_DATA_POINTS} days\")\n",
    "print(f\"- Features: {NUM_VARIABLES} ({', '.join(OHLCV_COLUMNS)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PSformer Model Implementation\n",
    "\n",
    "The following cells contain the complete source code for the PSformer model, adapted for the notebook environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== PS BLOCK IMPLEMENTATION ==========\n",
    "class PSBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Parameter Shared Block implementing Equation 3 from PSformer paper:\n",
    "    Xout = (GeLU(XinW(1))W(2) + Xin)W(3)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, N: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            N: Dimension size for N√óN weight matrices\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        \n",
    "        # Three N√óN linear layers with bias\n",
    "        self.linear1 = nn.Linear(N, N)\n",
    "        self.linear2 = nn.Linear(N, N) \n",
    "        self.linear3 = nn.Linear(N, N)\n",
    "        \n",
    "        # Activation function\n",
    "        self.activation = nn.GELU()\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize weights using Xavier initialization for W1, W2 and smaller weights for W3\"\"\"\n",
    "        nn.init.xavier_uniform_(self.linear1.weight)\n",
    "        nn.init.xavier_uniform_(self.linear2.weight)\n",
    "        # Initialize linear3 with smaller weights as it's the final transformation\n",
    "        nn.init.xavier_uniform_(self.linear3.weight, gain=0.1)\n",
    "        \n",
    "        if self.linear1.bias is not None:\n",
    "            nn.init.zeros_(self.linear1.bias)\n",
    "        if self.linear2.bias is not None:\n",
    "            nn.init.zeros_(self.linear2.bias)\n",
    "        if self.linear3.bias is not None:\n",
    "            nn.init.zeros_(self.linear3.bias)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass implementing the three-step transformation\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (C, N) or (batch, C, N)\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor of same shape as input\n",
    "        \"\"\"\n",
    "        # Handle both 2D and 3D tensors\n",
    "        original_shape = x.shape\n",
    "        is_3d = x.dim() == 3\n",
    "        \n",
    "        # Validate input shape\n",
    "        if x.dim() not in [2, 3]:\n",
    "            raise ValueError(f\"Input tensor must be 2 or 3-dimensional, got {x.dim()}\")\n",
    "        \n",
    "        if is_3d:\n",
    "            # Reshape 3D to 2D: [batch, C, N] -> [batch*C, N]\n",
    "            batch, C, N = x.shape\n",
    "            if N != self.N:\n",
    "                raise ValueError(f\"Input tensor last dimension must be {self.N}, got {N}\")\n",
    "            x = x.view(-1, N)  # [batch*C, N]\n",
    "        else:\n",
    "            # 2D case\n",
    "            if x.shape[1] != self.N:\n",
    "                raise ValueError(f\"Input tensor second dimension must be {self.N}, got {x.shape[1]}\")\n",
    "        \n",
    "        # Store original input for residual connection\n",
    "        residual = x\n",
    "        \n",
    "        # First transformation: Linear -> GeLU -> Linear + Residual\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        intermediate_output = x + residual\n",
    "        \n",
    "        # Second transformation: Linear\n",
    "        final_output = self.linear3(intermediate_output)\n",
    "        \n",
    "        # Reshape back to original shape if needed\n",
    "        if is_3d:\n",
    "            final_output = final_output.view(batch, C, N)\n",
    "        \n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== REVIN IMPLEMENTATION ==========\n",
    "class RevIN(nn.Module):\n",
    "    def __init__(self, num_features: int, eps=1e-5, affine=True):\n",
    "        \"\"\"\n",
    "        :param num_features: the number of features or channels\n",
    "        :param eps: a value added for numerical stability\n",
    "        :param affine: if True, RevIN has learnable affine parameters\n",
    "        \"\"\"\n",
    "        super(RevIN, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.affine = affine\n",
    "        if self.affine:\n",
    "            self._init_params()\n",
    "\n",
    "    def forward(self, x, mode:str):\n",
    "        if mode == 'norm':\n",
    "            self._get_statistics(x)\n",
    "            x = self._normalize(x)\n",
    "        elif mode == 'denorm':\n",
    "            x = self._denormalize(x)\n",
    "        else: raise NotImplementedError\n",
    "        return x\n",
    "\n",
    "    def _init_params(self):\n",
    "        # initialize RevIN params: (C,)\n",
    "        self.affine_weight = nn.Parameter(torch.ones(self.num_features))\n",
    "        self.affine_bias = nn.Parameter(torch.zeros(self.num_features))\n",
    "\n",
    "    def _get_statistics(self, x):\n",
    "        if x.ndim != 3:\n",
    "            raise IndexError(f\"Expected 3D input tensor [batch, channels, length], got {x.ndim}D tensor\")\n",
    "        # Compute statistics over the time dimension (dim=2) for each channel separately\n",
    "        # Input shape: [batch, channels, time] -> statistics shape: [batch, channels, 1]\n",
    "        self.mean = torch.mean(x, dim=2, keepdim=True).detach()\n",
    "        self.stdev = torch.sqrt(torch.var(x, dim=2, keepdim=True, unbiased=False) + self.eps).detach()\n",
    "\n",
    "    def _normalize(self, x):\n",
    "        x = x - self.mean\n",
    "        x = x / self.stdev\n",
    "        if self.affine:\n",
    "            # Reshape affine parameters to broadcast properly\n",
    "            # For input [B, C, T] and statistics [B, 1, T], \n",
    "            # reshape affine params [C] to [1, C, 1] for broadcasting\n",
    "            weight = self.affine_weight.view(1, -1, 1)\n",
    "            bias = self.affine_bias.view(1, -1, 1)\n",
    "            x = x * weight\n",
    "            x = x + bias\n",
    "        return x\n",
    "\n",
    "    def _denormalize(self, x):\n",
    "        if self.affine:\n",
    "            # Reshape affine parameters to broadcast properly\n",
    "            weight = self.affine_weight.view(1, -1, 1)\n",
    "            bias = self.affine_bias.view(1, -1, 1)\n",
    "            x = x - bias\n",
    "            x = x / (weight + self.eps*self.eps)\n",
    "        x = x * self.stdev\n",
    "        x = x + self.mean\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== DATA TRANSFORMER IMPLEMENTATION ==========\n",
    "class DataTransformationConfig:\n",
    "    \"\"\"\n",
    "    Configuration class for data transformation parameters\n",
    "    \"\"\"\n",
    "    def __init__(self, patch_size: int, sequence_length: int, num_variables: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patch_size: Size of each temporal patch (P)\n",
    "            sequence_length: Total input sequence length (L)\n",
    "            num_variables: Number of time series variables (M)\n",
    "        \"\"\"\n",
    "        self.patch_size = patch_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.num_variables = num_variables\n",
    "        \n",
    "        # Validate and calculate derived parameters\n",
    "        self._validate()\n",
    "        self.num_patches = self.sequence_length // self.patch_size\n",
    "        self.segment_length = self.num_variables * self.patch_size\n",
    "    \n",
    "    def _validate(self):\n",
    "        \"\"\"Validate configuration parameters\"\"\"\n",
    "        if self.patch_size <= 0:\n",
    "            raise ValueError(f\"Patch size must be positive, got {self.patch_size}\")\n",
    "        if self.sequence_length % self.patch_size != 0:\n",
    "            raise ValueError(f\"Sequence length {self.sequence_length} must be divisible by patch size {self.patch_size}\")\n",
    "        if self.num_variables <= 0:\n",
    "            raise ValueError(f\"Number of variables must be positive, got {self.num_variables}\")\n",
    "\n",
    "\n",
    "class PSformerDataTransformer:\n",
    "    \"\"\"\n",
    "    Data transformation utility for PSformer that handles conversion between\n",
    "    standard time series format and PSformer's segment-based representation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            config: DataTransformationConfig instance with transformation parameters\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self._validate_configuration()\n",
    "    \n",
    "    def _validate_configuration(self):\n",
    "        \"\"\"Validate the configuration\"\"\"\n",
    "        if not isinstance(self.config, DataTransformationConfig):\n",
    "            raise TypeError(\"config must be an instance of DataTransformationConfig\")\n",
    "    \n",
    "    def create_patches(self, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Transform: [batch, variables, sequence] -> [batch, variables, num_patches, patch_size]\n",
    "        \"\"\"\n",
    "        # Input validation\n",
    "        if input_tensor.dim() != 3:\n",
    "            raise ValueError(f\"Input tensor must be 3-dimensional, got {input_tensor.dim()}\")\n",
    "            \n",
    "        batch, variables, sequence = input_tensor.shape\n",
    "        \n",
    "        if sequence != self.config.sequence_length:\n",
    "            raise ValueError(f\"Input sequence length {sequence} does not match configured length {self.config.sequence_length}\")\n",
    "            \n",
    "        if variables != self.config.num_variables:\n",
    "            raise ValueError(f\"Input variables count {variables} does not match configured count {self.config.num_variables}\")\n",
    "        \n",
    "        # Reshape sequence dimension to split into patches\n",
    "        # [batch, variables, sequence] -> [batch, variables, num_patches, patch_size]\n",
    "        patched = input_tensor.view(batch, variables, self.config.num_patches, self.config.patch_size)\n",
    "        \n",
    "        return patched\n",
    "    \n",
    "    def create_segments(self, patched_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Transform: [batch, variables, num_patches, patch_size] -> [batch, num_patches, segment_length]\n",
    "        \"\"\"\n",
    "        # Input validation\n",
    "        if patched_tensor.dim() != 4:\n",
    "            raise ValueError(f\"Patched tensor must be 4-dimensional, got {patched_tensor.dim()}\")\n",
    "            \n",
    "        batch, variables, num_patches, patch_size = patched_tensor.shape\n",
    "        \n",
    "        # Step 1: Transpose to put patches before variables\n",
    "        # [batch, variables, num_patches, patch_size] -> [batch, num_patches, variables, patch_size]\n",
    "        transposed = patched_tensor.transpose(1, 2)\n",
    "        \n",
    "        # Step 2: Reshape to create segments by concatenating all variables for each patch\n",
    "        # [batch, num_patches, variables, patch_size] -> [batch, num_patches, variables*patch_size]\n",
    "        segments = transposed.contiguous().view(batch, num_patches, self.config.segment_length)\n",
    "        \n",
    "        return segments\n",
    "    \n",
    "    def restore_shape(self, segment_tensor: torch.Tensor, target_sequence_length: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Transform: [batch, num_patches, segment_length] -> [batch, variables, sequence]\n",
    "        \"\"\"\n",
    "        # Input validation\n",
    "        if segment_tensor.dim() != 3:\n",
    "            raise ValueError(f\"Segment tensor must be 3-dimensional, got {segment_tensor.dim()}\")\n",
    "            \n",
    "        batch, num_patches, segment_length = segment_tensor.shape\n",
    "        \n",
    "        # Step 1: Reshape segments back to separate variables and patches\n",
    "        # [batch, num_patches, segment_length] -> [batch, num_patches, variables, patch_size]\n",
    "        reshaped = segment_tensor.view(batch, num_patches, self.config.num_variables, self.config.patch_size)\n",
    "        \n",
    "        # Step 2: Transpose to put variables before patches\n",
    "        # [batch, num_patches, variables, patch_size] -> [batch, variables, num_patches, patch_size]\n",
    "        transposed = reshaped.transpose(1, 2)\n",
    "        \n",
    "        # Step 3: Reshape to flatten patches back into sequence\n",
    "        # [batch, variables, num_patches, patch_size] -> [batch, variables, num_patches*patch_size]\n",
    "        output = transposed.contiguous().view(batch, self.config.num_variables, target_sequence_length)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def forward_transform(self, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Complete pipeline: input -> patches -> segments\n",
    "        \"\"\"\n",
    "        patches = self.create_patches(input_tensor)\n",
    "        segments = self.create_segments(patches)\n",
    "        return segments\n",
    "    \n",
    "    def inverse_transform(self, segment_tensor: torch.Tensor, target_sequence_length: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Complete pipeline: segments -> patches -> output\n",
    "        \"\"\"\n",
    "        return self.restore_shape(segment_tensor, target_sequence_length)\n",
    "\n",
    "\n",
    "def create_transformer_for_psformer(sequence_length: int, num_variables: int, patch_size: int) -> PSformerDataTransformer:\n",
    "    \"\"\"\n",
    "    Factory method with PSformer-specific defaults\n",
    "    \"\"\"\n",
    "    config = DataTransformationConfig(\n",
    "        patch_size=patch_size,\n",
    "        sequence_length=sequence_length,\n",
    "        num_variables=num_variables\n",
    "    )\n",
    "    return PSformerDataTransformer(config)\n",
    "\n",
    "\n",
    "def get_psformer_dimensions(transformer: PSformerDataTransformer) -> dict:\n",
    "    \"\"\"\n",
    "    Return the key dimensions that PSformer encoder needs\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'C': transformer.config.segment_length,      # This becomes the feature dimension for attention\n",
    "        'N': transformer.config.num_patches,         # This becomes the sequence dimension for attention\n",
    "        'segment_shape': (None, transformer.config.num_patches, transformer.config.segment_length)  # Final shape for PSformer input\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== ATTENTION MECHANISM IMPLEMENTATION ==========\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Scaled Dot-Product Attention mechanism as described in the Attention Is All You Need paper.\n",
    "    Implements: Attention(Q, K, V) = softmax(QK^T / sqrt(dk)) * V\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dropout_rate: float = 0.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dropout_rate: Dropout rate for attention weights\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout_rate) if dropout_rate > 0 else None\n",
    "        self.scale = None\n",
    "    \n",
    "    def forward(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Compute scaled dot-product attention.\n",
    "        \"\"\"\n",
    "        # Validate input dimensions\n",
    "        if Q.dim() != 3 or K.dim() != 3 or V.dim() != 3:\n",
    "            raise ValueError(\"Q, K, and V must all be 3-dimensional tensors\")\n",
    "        \n",
    "        if Q.shape[0] != K.shape[0] or Q.shape[0] != V.shape[0]:\n",
    "            raise ValueError(\"Batch dimensions of Q, K, and V must match\")\n",
    "            \n",
    "        if K.shape[1] != V.shape[1]:\n",
    "            raise ValueError(\"Number of keys in K must match number of values in V\")\n",
    "            \n",
    "        if Q.shape[2] != K.shape[2]:\n",
    "            raise ValueError(\"Embedding dimensions of Q and K must match\")\n",
    "        \n",
    "        # Get the embedding dimension\n",
    "        dk = K.shape[2]\n",
    "        self.scale = torch.sqrt(torch.tensor(dk, dtype=torch.float32, device=Q.device))\n",
    "        \n",
    "        # Compute attention scores: Q @ K^T\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1))  # [batch, num_queries, num_keys]\n",
    "        \n",
    "        # Scale the scores\n",
    "        scaled_scores = scores / self.scale\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        attention_weights = F.softmax(scaled_scores, dim=-1)\n",
    "        \n",
    "        # Apply dropout if specified\n",
    "        if self.dropout is not None:\n",
    "            attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        output = torch.matmul(attention_weights, V)  # [batch, num_queries, dv]\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "\n",
    "class SegmentAttentionStage(nn.Module):\n",
    "    \"\"\"\n",
    "    Single stage of segment attention using a shared PS Block to generate Q, K, V.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, ps_block: PSBlock, use_dropout: bool = False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            ps_block: Shared PS Block used to generate Q, K, V\n",
    "            use_dropout: Whether to use dropout in the attention mechanism\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.ps_block = ps_block\n",
    "        self.attention = ScaledDotProductAttention(dropout_rate=0.1 if use_dropout else 0.0)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass through the segment attention stage.\n",
    "        \"\"\"\n",
    "        # Validate input shape\n",
    "        if x.dim() != 3:\n",
    "            raise ValueError(f\"Input tensor must be 3-dimensional, got {x.dim()}\")\n",
    "        \n",
    "        batch, N, C = x.shape\n",
    "        \n",
    "        # Generate Q, K, V using the shared PS Block\n",
    "        # In PSformer, all three come from the same PS Block output\n",
    "        ps_output = self.ps_block(x)  # [batch, N, C]\n",
    "        \n",
    "        # According to paper: attention operates across C dimension (spatial-temporal features)\n",
    "        # Transpose to [batch, C, N] so attention is computed across C dimension\n",
    "        Q = ps_output.transpose(-2, -1).contiguous()  # [batch, C, N]\n",
    "        K = ps_output.transpose(-2, -1).contiguous()  # [batch, C, N] \n",
    "        V = ps_output.transpose(-2, -1).contiguous()  # [batch, C, N]\n",
    "        \n",
    "        # Apply attention - this will create [batch, C, C] attention matrix\n",
    "        attention_output, attention_weights = self.attention(Q, K, V)\n",
    "        \n",
    "        # Transpose back to [batch, N, C] to maintain output format\n",
    "        attention_output = attention_output.transpose(-2, -1).contiguous()  # [batch, N, C]\n",
    "        \n",
    "        return attention_output, attention_weights\n",
    "\n",
    "\n",
    "class TwoStageSegmentAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Two-stage segment attention mechanism as described in the PSformer paper.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, ps_block: PSBlock):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            ps_block: Shared PS Block used across both attention stages\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.ps_block = ps_block  # Single shared PS Block\n",
    "        self.stage1 = SegmentAttentionStage(ps_block)\n",
    "        self.stage2 = SegmentAttentionStage(ps_block)\n",
    "        self.activation = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Forward pass through the two-stage segment attention.\n",
    "        \"\"\"\n",
    "        # Validate input shape\n",
    "        if x.dim() != 3:\n",
    "            raise ValueError(f\"Input tensor must be 3-dimensional, got {x.dim()}\")\n",
    "        \n",
    "        # Stage 1\n",
    "        stage1_output, stage1_weights = self.stage1(x)\n",
    "        \n",
    "        # ReLU activation between stages\n",
    "        activated_output = self.activation(stage1_output)\n",
    "        \n",
    "        # Stage 2\n",
    "        stage2_output, stage2_weights = self.stage2(activated_output)\n",
    "        \n",
    "        return stage2_output, (stage1_weights, stage2_weights)\n",
    "\n",
    "\n",
    "class PSformerEncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Single layer of the PSformer encoder.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, ps_block: PSBlock):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            ps_block: Shared PS Block used in all components of this layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.ps_block = ps_block  # Shared across all components\n",
    "        self.two_stage_attention = TwoStageSegmentAttention(ps_block)\n",
    "        self.final_ps_block = ps_block  # Same instance for final transformation\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Forward pass through the PSformer encoder layer.\n",
    "        \"\"\"\n",
    "        # Validate input shape\n",
    "        if x.dim() != 3:\n",
    "            raise ValueError(f\"Input tensor must be 3-dimensional, got {x.dim()}\")\n",
    "        \n",
    "        batch, N, C = x.shape\n",
    "        \n",
    "        # Two-stage attention\n",
    "        attention_output, attention_weights = self.two_stage_attention(x)\n",
    "        \n",
    "        # Residual connection\n",
    "        residual_output = attention_output + x\n",
    "        \n",
    "        # Final PS Block processing\n",
    "        output = self.final_ps_block(residual_output)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "\n",
    "class PSformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete PSformer encoder with multiple layers.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_layers: int, segment_length: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_layers: Number of encoder layers\n",
    "            segment_length: Length of each segment (C = M * P)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Each layer has its own PS Block\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            ps_block = PSBlock(N=segment_length)\n",
    "            encoder_layer = PSformerEncoderLayer(ps_block)\n",
    "            self.layers.append(encoder_layer)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, list]:\n",
    "        \"\"\"\n",
    "        Forward pass through the PSformer encoder.\n",
    "        \"\"\"\n",
    "        # Validate input shape\n",
    "        if x.dim() != 3:\n",
    "            raise ValueError(f\"Input tensor must be 3-dimensional, got {x.dim()}\")\n",
    "        \n",
    "        attention_weights_list = []\n",
    "        \n",
    "        # Process through each layer\n",
    "        for layer in self.layers:\n",
    "            x, weights = layer(x)\n",
    "            attention_weights_list.append(weights)\n",
    "        \n",
    "        return x, attention_weights_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== PSFORMER MAIN MODEL IMPLEMENTATION ==========\n",
    "class PSformerConfig:\n",
    "    \"\"\"\n",
    "    Configuration class for PSformer model parameters\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 sequence_length: int,\n",
    "                 num_variables: int, \n",
    "                 patch_size: int,\n",
    "                 num_encoder_layers: int,\n",
    "                 prediction_length: int,\n",
    "                 affine_revin: bool = True,\n",
    "                 revin_eps: float = 1e-5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sequence_length: Total input sequence length (L)\n",
    "            num_variables: Number of time series variables (M)\n",
    "            patch_size: Size of each temporal patch (P)\n",
    "            num_encoder_layers: Number of PSformer encoder layers\n",
    "            prediction_length: Length of prediction horizon (F)\n",
    "            affine_revin: Whether to use learnable affine parameters in RevIN\n",
    "            revin_eps: Small value for numerical stability in RevIN\n",
    "        \"\"\"\n",
    "        self.sequence_length = sequence_length\n",
    "        self.num_variables = num_variables\n",
    "        self.patch_size = patch_size\n",
    "        self.num_encoder_layers = num_encoder_layers\n",
    "        self.prediction_length = prediction_length\n",
    "        self.affine_revin = affine_revin\n",
    "        self.revin_eps = revin_eps\n",
    "        \n",
    "        # Validate configuration\n",
    "        self._validate()\n",
    "    \n",
    "    def _validate(self):\n",
    "        \"\"\"Validate configuration parameters\"\"\"\n",
    "        if self.sequence_length % self.patch_size != 0:\n",
    "            raise ValueError(f\"Sequence length {self.sequence_length} must be divisible by patch size {self.patch_size}\")\n",
    "        if self.num_variables <= 0:\n",
    "            raise ValueError(f\"Number of variables must be positive, got {self.num_variables}\")\n",
    "        if self.patch_size <= 0:\n",
    "            raise ValueError(f\"Patch size must be positive, got {self.patch_size}\")\n",
    "        if self.num_encoder_layers <= 0:\n",
    "            raise ValueError(f\"Number of encoder layers must be positive, got {self.num_encoder_layers}\")\n",
    "        if self.prediction_length <= 0:\n",
    "            raise ValueError(f\"Prediction length must be positive, got {self.prediction_length}\")\n",
    "\n",
    "\n",
    "class PSformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Main PSformer model implementing the complete input processing pipeline.\n",
    "    \n",
    "    Architecture: Raw Input ‚Üí RevIN Normalization ‚Üí Data Transformation ‚Üí PSformer Encoder\n",
    "    \n",
    "    Based on the PSformer paper architecture described in Section 3.2 and Figures 1 & 2.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: PSformerConfig):\n",
    "        \"\"\"\n",
    "        Initialize PSformer model with all components.\n",
    "        \n",
    "        Args:\n",
    "            config: PSformerConfig instance containing model parameters\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # 1. Instantiate the Reversible Instance Normalization (RevIN) layer\n",
    "        # It normalizes each variable independently\n",
    "        self.revin_layer = RevIN(\n",
    "            num_features=config.num_variables,\n",
    "            eps=config.revin_eps,\n",
    "            affine=config.affine_revin\n",
    "        )\n",
    "        \n",
    "        # 2. Instantiate the Data Transformer\n",
    "        # This handles patching and segment creation\n",
    "        self.data_transformer = create_transformer_for_psformer(\n",
    "            sequence_length=config.sequence_length,\n",
    "            num_variables=config.num_variables,\n",
    "            patch_size=config.patch_size\n",
    "        )\n",
    "        \n",
    "        # 3. Get key dimensions from the data transformer\n",
    "        # This ensures the encoder is built with correct C and N dimensions\n",
    "        psformer_dims = get_psformer_dimensions(self.data_transformer)\n",
    "        # C = segment_length = M * P\n",
    "        # N = num_patches = L / P\n",
    "        \n",
    "        # 4. Instantiate the PSformer Encoder\n",
    "        # The encoder uses the segment_length (C) as the feature dimension for attention\n",
    "        self.encoder = PSformerEncoder(\n",
    "            num_layers=config.num_encoder_layers,\n",
    "            segment_length=psformer_dims['C']\n",
    "        )\n",
    "        \n",
    "        # 5. Add the final linear projection layer for forecasting\n",
    "        # This layer maps the sequence length (L) to the prediction length (F)\n",
    "        # Paper reference: X_pred = X_out * W_F where W_F ‚àà R^(L√óF)\n",
    "        self.output_projection = nn.Linear(\n",
    "            in_features=config.sequence_length,\n",
    "            out_features=config.prediction_length\n",
    "        )\n",
    "        \n",
    "        # Store dimensions for easy access\n",
    "        self.psformer_dims = psformer_dims\n",
    "    \n",
    "    def forward(self, raw_input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass implementing the complete pipeline: \n",
    "        Input -> Normalization -> Transformation -> Encoder -> Inverse Transformation -> Projection -> Inverse Normalization -> Output\n",
    "        \n",
    "        Args:\n",
    "            raw_input_tensor: Input tensor of shape [batch_size, num_variables, sequence_length]\n",
    "                             or [batch, M, L] as per the paper notation\n",
    "        \n",
    "        Returns:\n",
    "            final_predictions: Tensor of shape [batch, M, F] where F is prediction_length\n",
    "        \"\"\"\n",
    "        # Validate input tensor\n",
    "        self._validate_input(raw_input_tensor)\n",
    "        \n",
    "        # ----- START OF INPUT PROCESSING PIPELINE -----\n",
    "        \n",
    "        # STEP 1: NORMALIZATION\n",
    "        # Apply RevIN layer in 'norm' mode to the raw input\n",
    "        # This is the first step shown in Figure 1 and mentioned in Section 3.2\n",
    "        # The statistics (mean, stdev) are automatically stored inside self.revin_layer\n",
    "        normalized_input = self.revin_layer(raw_input_tensor, mode='norm')\n",
    "        \n",
    "        # STEP 2: DATA TRANSFORMATION (Patching & Segmenting)\n",
    "        # Use the data_transformer's forward_transform method\n",
    "        # This transforms the input from [batch, M, L] to [batch, N, C]\n",
    "        encoder_ready_data = self.data_transformer.forward_transform(normalized_input)\n",
    "        \n",
    "        # STEP 3: ENCODER PROCESSING\n",
    "        # Feed the prepared data into the encoder\n",
    "        # The encoder returns its final output and a list of attention weights\n",
    "        encoder_output, attention_weights_list = self.encoder(encoder_ready_data)\n",
    "        \n",
    "        # ----- END OF INPUT PROCESSING, START OF OUTPUT PIPELINE -----\n",
    "        \n",
    "        # STEP 4.1: INVERSE DATA TRANSFORMATION\n",
    "        # Description: Convert the encoder's segmented output back to time series format.\n",
    "        # Paper reference: \"Inverse Transformation\" block in Fig 1 & 2.\n",
    "        reshaped_output = self.data_transformer.inverse_transform(\n",
    "            encoder_output, \n",
    "            self.config.sequence_length\n",
    "        )  # Shape: [B, M, L]\n",
    "        \n",
    "        # STEP 4.2: LINEAR PROJECTION FOR FORECASTING\n",
    "        # Description: Project the restored sequence to the desired prediction horizon.\n",
    "        # Paper reference: \"Linear Mapping\" (Fig 1) or WF matrix multiplication (Section 3.2).\n",
    "        projected_output = self.output_projection(reshaped_output)  # Shape: [B, M, F]\n",
    "        \n",
    "        # STEP 4.3: INVERSE NORMALIZATION (RevIN Denorm)\n",
    "        # Description: Scale the forecast back to the original data distribution.\n",
    "        # Paper reference: \"Inverse RevIN\" (Fig 1) or \"RevIN‚Åª¬π\" (Fig 2).\n",
    "        final_predictions = self.revin_layer(projected_output, mode='denorm')  # Shape: [B, M, F]\n",
    "        \n",
    "        return final_predictions\n",
    "    \n",
    "    def _validate_input(self, input_tensor: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Validate the input tensor shape and properties.\n",
    "        \"\"\"\n",
    "        if input_tensor.dim() != 3:\n",
    "            raise ValueError(f\"Input tensor must be 3-dimensional [batch, variables, sequence], got {input_tensor.dim()}D tensor\")\n",
    "        \n",
    "        batch, variables, sequence = input_tensor.shape\n",
    "        \n",
    "        if variables != self.config.num_variables:\n",
    "            raise ValueError(f\"Input variables count {variables} does not match configured count {self.config.num_variables}\")\n",
    "        \n",
    "        if sequence != self.config.sequence_length:\n",
    "            raise ValueError(f\"Input sequence length {sequence} does not match configured length {self.config.sequence_length}\")\n",
    "\n",
    "\n",
    "def create_psformer_model(sequence_length: int, \n",
    "                         num_variables: int, \n",
    "                         patch_size: int, \n",
    "                         num_encoder_layers: int,\n",
    "                         prediction_length: int,\n",
    "                         **kwargs) -> PSformer:\n",
    "    \"\"\"\n",
    "    Factory function to create a PSformer model with default configuration.\n",
    "    \"\"\"\n",
    "    config = PSformerConfig(\n",
    "        sequence_length=sequence_length,\n",
    "        num_variables=num_variables,\n",
    "        patch_size=patch_size,\n",
    "        num_encoder_layers=num_encoder_layers,\n",
    "        prediction_length=prediction_length,\n",
    "        **kwargs\n",
    "    )\n",
    "    return PSformer(config)\n",
    "\n",
    "print(\"‚úÖ PSformer model implementation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Function for Data Splitting\n",
    "\n",
    "This function separates the time series of each stock into two distinct parts:\n",
    "1. **Model Input:** The historical data the model will use to make a forecast\n",
    "2. **Validation Ground Truth:** The most recent 30 days of data, which the model will not see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_ticker_data(ticker_df: pd.DataFrame) -> tuple:\n",
    "    \"\"\"\n",
    "    Prepare data for a single ticker by splitting into model input and validation sets.\n",
    "    \n",
    "    Args:\n",
    "        ticker_df: DataFrame containing data for a single ticker\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (model_input_df, validation_ground_truth_df) or (None, None) if insufficient data\n",
    "    \"\"\"\n",
    "    # Check if there's enough data\n",
    "    if len(ticker_df) < MIN_DATA_POINTS:\n",
    "        return None, None\n",
    "    \n",
    "    # Sort by date to ensure chronological order\n",
    "    ticker_df = ticker_df.sort_values(DATE_COLUMN).reset_index(drop=True)\n",
    "    \n",
    "    # Split the data\n",
    "    # Validation set: last PREDICTION_LENGTH rows\n",
    "    validation_ground_truth = ticker_df.tail(PREDICTION_LENGTH).copy()\n",
    "    \n",
    "    # Model input set: SEQUENCE_LENGTH rows just before the validation set\n",
    "    end_idx = len(ticker_df) - PREDICTION_LENGTH\n",
    "    start_idx = end_idx - SEQUENCE_LENGTH\n",
    "    \n",
    "    if start_idx < 0:\n",
    "        return None, None\n",
    "    \n",
    "    model_input = ticker_df.iloc[start_idx:end_idx].copy()\n",
    "    \n",
    "    return model_input, validation_ground_truth\n",
    "\n",
    "\n",
    "def prepare_tensor_from_dataframe(df: pd.DataFrame) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Convert DataFrame to PyTorch tensor in the format expected by PSformer.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with OHLCV columns\n",
    "        \n",
    "    Returns:\n",
    "        Tensor of shape [1, num_variables, sequence_length]\n",
    "    \"\"\"\n",
    "    # Extract OHLCV values\n",
    "    values = df[OHLCV_COLUMNS].values  # Shape: [sequence_length, num_variables]\n",
    "    \n",
    "    # Convert to tensor and transpose\n",
    "    tensor = torch.tensor(values, dtype=torch.float32)  # [sequence_length, num_variables]\n",
    "    tensor = tensor.transpose(0, 1)  # [num_variables, sequence_length]\n",
    "    tensor = tensor.unsqueeze(0)  # [1, num_variables, sequence_length]\n",
    "    \n",
    "    return tensor.to(DEVICE)\n",
    "\n",
    "\n",
    "def tensor_to_dataframe(tensor: torch.Tensor, dates: pd.DatetimeIndex, ticker: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert prediction tensor back to DataFrame format.\n",
    "    \n",
    "    Args:\n",
    "        tensor: Prediction tensor of shape [1, num_variables, prediction_length]\n",
    "        dates: Date index for the predictions\n",
    "        ticker: Ticker symbol\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with prediction results\n",
    "    \"\"\"\n",
    "    # Convert tensor to numpy and transpose\n",
    "    predictions = tensor.squeeze(0).cpu().numpy()  # [num_variables, prediction_length]\n",
    "    predictions = predictions.transpose()  # [prediction_length, num_variables]\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(predictions, columns=[f\"{col}_predicted\" for col in OHLCV_COLUMNS])\n",
    "    df[DATE_COLUMN] = dates\n",
    "    df[TICKER_COLUMN] = ticker\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"‚úÖ Data preparation functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Prediction and Validation Loop\n",
    "\n",
    "This loop performs a backtest for each ticker by forecasting the hold-out period and comparing it to the actual known data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the main CSV file\n",
    "try:\n",
    "    print(f\"Loading data from {DATA_FILE_PATH}...\")\n",
    "    df = pd.read_csv(DATA_FILE_PATH)\n",
    "    print(f\"Data loaded successfully! Shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Convert date column to datetime\n",
    "    df[DATE_COLUMN] = pd.to_datetime(df[DATE_COLUMN])\n",
    "    \n",
    "    # Get unique tickers\n",
    "    unique_tickers = df[TICKER_COLUMN].unique()\n",
    "    print(f\"Found {len(unique_tickers)} unique tickers: {list(unique_tickers)[:10]}{'...' if len(unique_tickers) > 10 else ''}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading data: {e}\")\n",
    "    print(\"Please make sure your CSV file is uploaded and the path is correct.\")\n",
    "    raise\n",
    "\n",
    "# Initialize results storage\n",
    "all_validation_results = []\n",
    "successful_predictions = 0\n",
    "skipped_tickers = []\n",
    "\n",
    "print(f\"\\nüöÄ Starting prediction loop for {len(unique_tickers)} tickers...\")\n",
    "print(f\"Minimum data requirement: {MIN_DATA_POINTS} days\\n\")\n",
    "\n",
    "# Loop through each ticker\n",
    "for i, ticker in enumerate(unique_tickers):\n",
    "    print(f\"[{i+1}/{len(unique_tickers)}] Processing {ticker}...\", end=\" \")\n",
    "    \n",
    "    try:\n",
    "        # Get data for current ticker\n",
    "        ticker_data = df[df[TICKER_COLUMN] == ticker].copy()\n",
    "        \n",
    "        # Prepare data splits\n",
    "        model_input, ground_truth = prepare_ticker_data(ticker_data)\n",
    "        \n",
    "        # Check if we have enough data\n",
    "        if model_input is None or ground_truth is None:\n",
    "            print(f\"‚ùå Insufficient data ({len(ticker_data)} days)\")\n",
    "            skipped_tickers.append(ticker)\n",
    "            continue\n",
    "        \n",
    "        # Create PSformer model (with random weights)\n",
    "        config = PSformerConfig(\n",
    "            sequence_length=SEQUENCE_LENGTH,\n",
    "            num_variables=NUM_VARIABLES,\n",
    "            patch_size=PATCH_SIZE,\n",
    "            num_encoder_layers=NUM_ENCODER_LAYERS,\n",
    "            prediction_length=PREDICTION_LENGTH\n",
    "        )\n",
    "        model = PSformer(config).to(DEVICE)\n",
    "        model.eval()  # Set to evaluation mode\n",
    "        \n",
    "        # Prepare input tensor\n",
    "        input_tensor = prepare_tensor_from_dataframe(model_input)\n",
    "        \n",
    "        # Make prediction\n",
    "        with torch.no_grad():\n",
    "            prediction_tensor = model(input_tensor)\n",
    "        \n",
    "        # Generate future dates for predictions\n",
    "        last_date = ground_truth[DATE_COLUMN].iloc[0]\n",
    "        prediction_dates = pd.date_range(start=last_date, periods=PREDICTION_LENGTH, freq='D')\n",
    "        \n",
    "        # Convert prediction to DataFrame\n",
    "        prediction_df = tensor_to_dataframe(prediction_tensor, prediction_dates, ticker)\n",
    "        \n",
    "        # Prepare ground truth for comparison\n",
    "        ground_truth_comparison = ground_truth[[DATE_COLUMN, TICKER_COLUMN] + OHLCV_COLUMNS].copy()\n",
    "        ground_truth_comparison.columns = [DATE_COLUMN, TICKER_COLUMN] + [f\"{col}_actual\" for col in OHLCV_COLUMNS]\n",
    "        \n",
    "        # Merge predictions with ground truth\n",
    "        comparison_df = pd.merge(ground_truth_comparison, prediction_df, on=[DATE_COLUMN, TICKER_COLUMN], how='inner')\n",
    "        \n",
    "        # Add to results\n",
    "        all_validation_results.append(comparison_df)\n",
    "        successful_predictions += 1\n",
    "        \n",
    "        print(f\"‚úÖ Success ({len(model_input)} -> {len(ground_truth)} days)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        skipped_tickers.append(ticker)\n",
    "        continue\n",
    "\n",
    "print(f\"\\nüìä Prediction Summary:\")\n",
    "print(f\"- Successful predictions: {successful_predictions}\")\n",
    "print(f\"- Skipped tickers: {len(skipped_tickers)}\")\n",
    "if skipped_tickers:\n",
    "    print(f\"- Skipped: {skipped_tickers[:5]}{'...' if len(skipped_tickers) > 5 else ''}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze and Visualize Validation Results\n",
    "\n",
    "This final section brings all the results together and provides both quantitative and visual analysis of the model's performance on the hold-out data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(all_validation_results) == 0:\n",
    "    print(\"‚ùå No successful predictions to analyze!\")\n",
    "else:\n",
    "    # ========== CONSOLIDATE RESULTS ==========\n",
    "    print(\"üìà Consolidating results...\")\n",
    "    consolidated_results = pd.concat(all_validation_results, ignore_index=True)\n",
    "    print(f\"Total predictions: {len(consolidated_results)} rows\")\n",
    "    print(f\"Date range: {consolidated_results[DATE_COLUMN].min()} to {consolidated_results[DATE_COLUMN].max()}\")\n",
    "    \n",
    "    # ========== QUANTITATIVE ANALYSIS ==========\n",
    "    print(\"\\nüìä Calculating error metrics...\")\n",
    "    \n",
    "    # Calculate MAE for each ticker and feature\n",
    "    error_metrics = []\n",
    "    \n",
    "    for ticker in consolidated_results[TICKER_COLUMN].unique():\n",
    "        ticker_data = consolidated_results[consolidated_results[TICKER_COLUMN] == ticker]\n",
    "        \n",
    "        ticker_metrics = {'Ticker': ticker}\n",
    "        \n",
    "        for feature in OHLCV_COLUMNS:\n",
    "            actual_col = f\"{feature}_actual\"\n",
    "            predicted_col = f\"{feature}_predicted\"\n",
    "            \n",
    "            if actual_col in ticker_data.columns and predicted_col in ticker_data.columns:\n",
    "                mae = np.mean(np.abs(ticker_data[actual_col] - ticker_data[predicted_col]))\n",
    "                ticker_metrics[f\"{feature}_MAE\"] = mae\n",
    "        \n",
    "        error_metrics.append(ticker_metrics)\n",
    "    \n",
    "    error_df = pd.DataFrame(error_metrics)\n",
    "    \n",
    "    # Display summary statistics\n",
    "    print(\"\\nüìã Error Summary (Mean Absolute Error):\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for feature in OHLCV_COLUMNS:\n",
    "        mae_col = f\"{feature}_MAE\"\n",
    "        if mae_col in error_df.columns:\n",
    "            mean_mae = error_df[mae_col].mean()\n",
    "            std_mae = error_df[mae_col].std()\n",
    "            print(f\"{feature:>6}: {mean_mae:.4f} ¬± {std_mae:.4f}\")\n",
    "    \n",
    "    # Show top 5 and bottom 5 performers for Close price\n",
    "    if 'Close_MAE' in error_df.columns:\n",
    "        print(\"\\nüèÜ Best Close Price Predictions (Lowest MAE):\")\n",
    "        best_performers = error_df.nsmallest(5, 'Close_MAE')[['Ticker', 'Close_MAE']]\n",
    "        print(best_performers.to_string(index=False))\n",
    "        \n",
    "        print(\"\\nüìâ Worst Close Price Predictions (Highest MAE):\")\n",
    "        worst_performers = error_df.nlargest(5, 'Close_MAE')[['Ticker', 'Close_MAE']]\n",
    "        print(worst_performers.to_string(index=False))\n",
    "    \n",
    "    # ========== VISUAL ANALYSIS ==========\n",
    "    print(\"\\nüìä Creating visualizations...\")\n",
    "    \n",
    "    # Select a few tickers for detailed visualization\n",
    "    sample_tickers = consolidated_results[TICKER_COLUMN].unique()[:3]  # First 3 tickers\n",
    "    \n",
    "    # Create subplot for each ticker\n",
    "    fig, axes = plt.subplots(len(sample_tickers), 1, figsize=(12, 4 * len(sample_tickers)))\n",
    "    if len(sample_tickers) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, ticker in enumerate(sample_tickers):\n",
    "        ticker_data = consolidated_results[consolidated_results[TICKER_COLUMN] == ticker]\n",
    "        ticker_data = ticker_data.sort_values(DATE_COLUMN)\n",
    "        \n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Plot actual vs predicted Close prices\n",
    "        ax.plot(ticker_data[DATE_COLUMN], ticker_data['Close_actual'], \n",
    "                label='Actual Close', color='blue', linewidth=2, marker='o')\n",
    "        ax.plot(ticker_data[DATE_COLUMN], ticker_data['Close_predicted'], \n",
    "                label='Predicted Close', color='red', linewidth=2, marker='s')\n",
    "        \n",
    "        ax.set_title(f'{ticker} - Actual vs Predicted Close Price', fontsize=14, fontweight='bold')\n",
    "        ax.set_xlabel('Date')\n",
    "        ax.set_ylabel('Price')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Rotate x-axis labels for better readability\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Interactive plot with Plotly (if available)\n",
    "    try:\n",
    "        print(\"\\nüìä Creating interactive plot...\")\n",
    "        \n",
    "        # Select one ticker for detailed interactive analysis\n",
    "        sample_ticker = sample_tickers[0]\n",
    "        sample_data = consolidated_results[consolidated_results[TICKER_COLUMN] == sample_ticker].sort_values(DATE_COLUMN)\n",
    "        \n",
    "        fig_plotly = go.Figure()\n",
    "        \n",
    "        # Add actual prices\n",
    "        fig_plotly.add_trace(go.Scatter(\n",
    "            x=sample_data[DATE_COLUMN],\n",
    "            y=sample_data['Close_actual'],\n",
    "            mode='lines+markers',\n",
    "            name='Actual Close',\n",
    "            line=dict(color='blue', width=2)\n",
    "        ))\n",
    "        \n",
    "        # Add predicted prices\n",
    "        fig_plotly.add_trace(go.Scatter(\n",
    "            x=sample_data[DATE_COLUMN],\n",
    "            y=sample_data['Close_predicted'],\n",
    "            mode='lines+markers',\n",
    "            name='Predicted Close',\n",
    "            line=dict(color='red', width=2, dash='dash')\n",
    "        ))\n",
    "        \n",
    "        fig_plotly.update_layout(\n",
    "            title=f'{sample_ticker} - Interactive Actual vs Predicted Close Price',\n",
    "            xaxis_title='Date',\n",
    "            yaxis_title='Price',\n",
    "            hovermode='x unified'\n",
    "        )\n",
    "        \n",
    "        fig_plotly.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Note: Interactive plot not available: {e}\")\n",
    "    \n",
    "    # ========== SAVE RESULTS ==========\n",
    "    print(\"\\nüíæ Saving results...\")\n",
    "    \n",
    "    # Save consolidated validation results\n",
    "    validation_filename = 'validation_results.csv'\n",
    "    consolidated_results.to_csv(validation_filename, index=False)\n",
    "    print(f\"‚úÖ Validation results saved to: {validation_filename}\")\n",
    "    \n",
    "    # Save error metrics\n",
    "    error_filename = 'error_metrics.csv'\n",
    "    error_df.to_csv(error_filename, index=False)\n",
    "    print(f\"‚úÖ Error metrics saved to: {error_filename}\")\n",
    "    \n",
    "    print(\"\\nüéâ Analysis complete!\")\n",
    "    print(f\"\\nüìà Summary:\")\n",
    "    print(f\"- Processed {len(sample_tickers)} tickers for visualization\")\n",
    "    print(f\"- Total validation samples: {len(consolidated_results)}\")\n",
    "    print(f\"- Files saved: {validation_filename}, {error_filename}\")\n",
    "    \n",
    "    # Display sample of results\n",
    "    print(\"\\nüìã Sample Results:\")\n",
    "    print(consolidated_results.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary and Next Steps\n",
    "\n",
    "## What We Accomplished\n",
    "\n",
    "‚úÖ **Model Implementation**: Complete PSformer architecture with all components  \n",
    "‚úÖ **Multi-ticker Processing**: Automated prediction pipeline for multiple stocks  \n",
    "‚úÖ **Backtest Validation**: Proper train/test split with held-out validation data  \n",
    "‚úÖ **Error Analysis**: Quantitative metrics (MAE) for each ticker and feature  \n",
    "‚úÖ **Visualization**: Actual vs predicted price comparisons  \n",
    "‚úÖ **Data Export**: Results saved for further analysis  \n",
    "\n",
    "## Important Notes\n",
    "\n",
    "‚ö†Ô∏è **Random Weights**: This model uses untrained, random weights for demonstration purposes  \n",
    "‚ö†Ô∏è **Training Required**: For real-world use, the model needs proper training on historical data  \n",
    "‚ö†Ô∏è **Validation Results**: Current predictions are random and should not be used for trading  \n",
    "\n",
    "## Next Steps for Real Implementation\n",
    "\n",
    "1. **Data Collection**: Gather comprehensive historical stock data\n",
    "2. **Model Training**: Implement proper training loop with loss functions\n",
    "3. **Hyperparameter Tuning**: Optimize sequence length, patch size, etc.\n",
    "4. **Cross-validation**: Implement rolling window validation\n",
    "5. **Feature Engineering**: Add technical indicators, market sentiment\n",
    "6. **Risk Management**: Implement position sizing and stop-loss mechanisms\n",
    "\n",
    "## Files Generated\n",
    "\n",
    "- `validation_results.csv`: Detailed predictions vs actual values\n",
    "- `error_metrics.csv`: Performance metrics by ticker\n",
    "- This notebook: Complete implementation ready for training\n",
    "\n",
    "---\n",
    "\n",
    "*This notebook demonstrates the PSformer architecture and provides a foundation for building a production-ready stock forecasting system.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
