## Attention Mechanism Implementation Plan

Based on the PSformer paper Section 3.2 and current project status, here's the implementation plan:

### Phase 1: Basic Attention Components

**Step 2.1: Scaled Dot-Product Attention**
```pseudo
CLASS ScaledDotProductAttention:
    INIT(dropout_rate=0.0):
        self.dropout = Dropout(dropout_rate)
        self.scale = None  # Will be set dynamically
    
    FORWARD(Q, K, V):
        # Q, K, V shape: [batch, N, C]
        dk = K.shape[-1]
        self.scale = sqrt(dk)
        
        # Compute attention scores
        scores = matmul(Q, K.transpose(-2, -1)) / self.scale
        attention_weights = softmax(scores, dim=-1)
        attention_weights = self.dropout(attention_weights)
        
        # Apply attention to values
        output = matmul(attention_weights, V)
        RETURN output, attention_weights
```

**Step 2.2: Single Stage Segment Attention**
```pseudo
CLASS SegmentAttentionStage:
    INIT(ps_block: PSBlock, use_dropout=False):
        self.ps_block = ps_block  # Shared PS Block
        self.attention = ScaledDotProductAttention(dropout_rate=0.1 if use_dropout else 0.0)
    
    FORWARD(x):
        # x shape: [batch, N, C] where C = M*P
        # Generate Q, K, V using shared PS Block
        ps_output = self.ps_block(x)  # [batch, N, C]
        
        # Use same output for Q, K, V (key PSformer innovation)
        Q = ps_output
        K = ps_output  
        V = ps_output
        
        # Apply attention
        attention_output, attention_weights = self.attention(Q, K, V)
        RETURN attention_output, attention_weights
```

### Phase 2: Two-Stage Segment Attention

**Step 2.3: Complete Two-Stage SegAtt**
```pseudo
CLASS TwoStageSegmentAttention:
    INIT(ps_block: PSBlock):
        self.ps_block = ps_block  # Single shared PS Block
        self.stage1 = SegmentAttentionStage(ps_block)
        self.stage2 = SegmentAttentionStage(ps_block)
        self.activation = ReLU()
    
    FORWARD(x):
        # Stage 1
        stage1_output, stage1_weights = self.stage1(x)
        
        # ReLU activation between stages
        activated_output = self.activation(stage1_output)
        
        # Stage 2
        stage2_output, stage2_weights = self.stage2(activated_output)
        
        RETURN stage2_output, (stage1_weights, stage2_weights)
```

### Phase 3: PSformer Encoder Integration

**Step 2.4: Single Encoder Layer**
```pseudo
CLASS PSformerEncoderLayer:
    INIT(ps_block: PSBlock):
        self.ps_block = ps_block  # Shared across all components
        self.two_stage_attention = TwoStageSegmentAttention(ps_block)
        self.final_ps_block = ps_block  # Same instance
    
    FORWARD(x):
        # x shape: [batch, N, C]
        
        # Two-stage attention
        attention_output, attention_weights = self.two_stage_attention(x)
        
        # Residual connection
        residual_output = attention_output + x
        
        # Final PS Block processing
        output = self.final_ps_block(residual_output)
        
        RETURN output, attention_weights
```

**Step 2.5: Complete PSformer Encoder**
```pseudo
CLASS PSformerEncoder:
    INIT(num_layers: int, segment_length: int):
        # Each layer has its own PS Block
        self.layers = []
        FOR i in range(num_layers):
            ps_block = PSBlock(N=segment_length)
            encoder_layer = PSformerEncoderLayer(ps_block)
            self.layers.append(encoder_layer)
    
    FORWARD(x):
        # x shape: [batch, N, C] from data transformer
        attention_weights_list = []
        
        FOR layer in self.layers:
            x, weights = layer(x)
            attention_weights_list.append(weights)
        
        RETURN x, attention_weights_list
```

### Testing Strategy

```pseudo
TEST_DIMENSIONS:
    # Based on paper's example: M=7, P=16, N=32
    C = 112  # M * P
    N = 32   # num_segments
    
    VERIFY:
    - Attention weights shape: [batch, N, N]
    - Attention weights sum to 1.0 across dim=-1
    - Output preserves shape: [batch, N, C]

TEST_GRADIENT_FLOW:
    - Ensure gradients flow through both attention stages
    - Verify PS Block weights update correctly
    - Check residual connections preserve gradients

TEST_INTEGRATION:
    # With existing components
    data_transformer = PSformerDataTransformer(config)
    segments = data_transformer.forward_transform(input)  # [batch, N, C]
    encoder = PSformerEncoder(num_layers=1, segment_length=N)
    output, weights = encoder(segments)
    ASSERT output.shape == segments.shape
```

### Key Implementation Notes

1. **Parameter Sharing**: The same PS Block instance is used for:
   - Generating Q, K, V in stage 1
   - Generating Q, K, V in stage 2  
   - Final transformation after residual

2. **Attention Computation**: Following Equation (5) in paper:
   - `Attention = Softmax(QK^T/√dk)V`
   - Q, K, V all come from same PS Block output

3. **Dimension Handling**:
   - Input: [batch, N, C] where C = M×P
   - Attention operates on N dimension (segments)
   - C dimension represents combined spatial-temporal features

This plan provides the foundation for implementing PSformer's core attention mechanism while maintaining compatibility with existing components.