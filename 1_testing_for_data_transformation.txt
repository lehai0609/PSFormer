# TDD Planning for Data Transformation Utilities

## Core Testing Strategy

Based on Section 3.2, the transformation flow is: **X ∈ R^(M×L) → patches → segments → X ∈ R^(C×N)** where C = M × P.

## Test Categories

### 1. Unit Tests - Atomic Transformations

```pseudo
CLASS TestDataTransformationAtomic:
    
    TEST_create_patches_basic():
        # Test: [batch=2, M=7, L=96] -> [batch=2, M=7, N=6, P=16]
        GIVEN: input_tensor = [2, 7, 96], patch_size=16
        WHEN: patches = create_patches(input_tensor)
        THEN: 
            ASSERT patches.shape == [2, 7, 6, 16]
            ASSERT patches[0, 0, 0, :] == input_tensor[0, 0, 0:16]
            ASSERT patches[0, 0, 1, :] == input_tensor[0, 0, 16:32]
    
    TEST_create_segments_basic():
        # Test: [batch=2, M=7, N=6, P=16] -> [batch=2, N=6, C=112]
        GIVEN: patches = [2, 7, 6, 16]
        WHEN: segments = create_segments(patches)
        THEN:
            ASSERT segments.shape == [2, 6, 112]  # C = 7*16 = 112
            ASSERT segments[0, 0, :] contains_all_variables_patch_0
    
    TEST_restore_shape_basic():
        # Test: [batch=2, N=6, C=112] -> [batch=2, M=7, L=96]
        GIVEN: segments = [2, 6, 112], target_length=96
        WHEN: output = restore_shape(segments, target_length)
        THEN: ASSERT output.shape == [2, 7, 96]
```

### 2. Integration Tests - PSformer Pipeline

```pseudo
CLASS TestDataTransformationIntegration:
    
    TEST_psformer_encoder_input_compatibility():
        # Test that transformed data works with PSformer Encoder
        GIVEN: 
            input_data = [batch=16, M=7, L=512]
            transformer = DataTransformer(patch_size=16)
            ps_block = PSBlock(N=32)  # N = L/P = 512/16 = 32
        
        WHEN: 
            segments = transformer.forward_transform(input_data)
            # segments shape should be [16, 32, 112] where C=7*16=112
            ps_output = ps_block(segments)
        
        THEN:
            ASSERT segments.shape == [16, 32, 112]
            ASSERT ps_output.shape == [16, 32, 112]  # PS Block preserves shape
    
    TEST_full_psformer_pipeline():
        # Test complete data flow: Input -> Transform -> PS Block -> Attention -> Restore
        GIVEN:
            original_input = [batch=16, M=7, L=512]
            transformer = DataTransformer(patch_size=16)
            
        WHEN:
            # Forward through PSformer
            segments = transformer.forward_transform(original_input)
            # Simulate PSformer processing (would be attention + PS blocks)
            processed_segments = mock_psformer_processing(segments)
            # Transform back for output
            restored = transformer.inverse_transform(processed_segments, L=512)
            
        THEN:
            ASSERT restored.shape == original_input.shape
            ASSERT_shapes_compatible_throughout_pipeline()
    
    TEST_revin_integration():
        # Test integration with RevIN normalization
        GIVEN:
            input_data = [batch=16, M=7, L=512]
            revin = RevIN(num_features=7)
            transformer = DataTransformer(patch_size=16)
            
        WHEN:
            normalized = revin(input_data, mode='norm')
            segments = transformer.forward_transform(normalized)
            restored_segments = transformer.inverse_transform(segments, L=512)
            denormalized = revin(restored_segments, mode='denorm')
            
        THEN:
            ASSERT torch.allclose(denormalized, input_data, atol=1e-5)
```

### 3. Property-Based Tests

```pseudo
CLASS TestDataTransformationProperties:
    
    TEST_transformation_symmetry():
        # Property: forward + inverse = identity
        FOR various_input_shapes IN [(16,7,512), (8,21,96), (32,1,224)]:
            GIVEN: input_tensor = random_tensor(various_input_shapes)
            WHEN: 
                segments = transformer.forward_transform(input_tensor)
                restored = transformer.inverse_transform(segments, input_tensor.shape[-1])
            THEN: 
                ASSERT torch.allclose(input_tensor, restored, atol=1e-6)
    
    TEST_element_preservation():
        # Property: no elements lost/gained in transformation
        GIVEN: input_tensor = random_tensor([16, 7, 512])
        WHEN: segments = transformer.forward_transform(input_tensor)
        THEN: 
            ASSERT torch.sum(input_tensor) ≈ torch.sum(segments)
            ASSERT torch.numel(input_tensor) == torch.numel(segments)
```

### 4. Dimension Compatibility Tests

```pseudo
CLASS TestDimensionCompatibility:
    
    TEST_psformer_encoder_dimensions():
        # Test dimensions match PSformer paper specifications
        FOR dataset_config IN [ETTh1, ETTm1, Weather, Traffic]:
            GIVEN: 
                M = dataset_config.num_variables
                L = 512  # Standard input length
                P = 16   # Standard patch size
                
            WHEN: 
                transformer = DataTransformer(patch_size=P)
                segments = transformer.forward_transform([1, M, L])
                
            THEN:
                N = L // P  # Number of segments
                C = M * P   # Segment length
                ASSERT segments.shape == [1, N, C]
                ASSERT C matches_ps_block_dimension()
    
    TEST_attention_mechanism_compatibility():
        # Test that segments work with scaled dot-product attention
        GIVEN: segments = [batch, N, C]
        WHEN: 
            Q = K = V = segments  # PS Block output used as Q,K,V
            attention_scores = torch.matmul(Q, K.transpose(-2, -1))
            
        THEN:
            ASSERT attention_scores.shape == [batch, N, N]
            ASSERT attention_mechanism_can_process(segments)
```

### 5. Error Handling Tests

```pseudo
CLASS TestDataTransformationErrors:
    
    TEST_invalid_dimensions():
        WITH pytest.raises(ValueError):
            # L not divisible by P
            transformer = DataTransformer(patch_size=17)
            transformer.forward_transform([16, 7, 512])  # 512 % 17 != 0
    
    TEST_mismatched_restore_dimensions():
        WITH pytest.raises(DimensionMismatchError):
            segments = random_tensor([16, 32, 112])
            transformer.inverse_transform(segments, target_length=100)  # Wrong length
```

## Integration Priority Matrix

```pseudo
INTEGRATION_TESTS_PRIORITY:
    1. HIGH: PSBlock compatibility (segments → PS Block → segments)
    2. HIGH: Attention mechanism input format (segments as Q,K,V)
    3. MEDIUM: RevIN integration (normalization before/after transform)
    4. MEDIUM: Multiple encoder layers (shape preservation)
    5. LOW: Linear output projection compatibility
```

## Test Execution Strategy

```pseudo
TDD_WORKFLOW:
    PHASE_1: Write failing unit tests for each transformation
    PHASE_2: Implement minimal transformation logic
    PHASE_3: Write integration tests with PSBlock
    PHASE_4: Implement complete transformer
    PHASE_5: Add property-based tests for robustness
    
CONTINUOUS_VALIDATION:
    RUN_on_each_commit: dimension_compatibility_tests()
    RUN_before_integration: psformer_pipeline_tests()
```

This testing strategy ensures the data transformation module integrates seamlessly with PSformer's core components while maintaining mathematical correctness.