# Implementation Deviation Report: PSformer Attention Mechanism

## Executive Summary

A single but significant deviation has been identified between the current PSformer implementation and the research paper specification in Section 3.2. The deviation concerns the dimensional orientation of the attention mechanism, which affects the fundamental operation of the Spatial-Temporal Segment Attention component.

## Detailed Analysis

### Paper Specification

The research paper clearly specifies the attention mechanism dimensions in Section 3.2 Model Structure:

> "In this transformed space, identical Q ∈ R^(C×N), K ∈ R^(C×N), and V ∈ R^(C×N) matrices are generated by applying a shared block's non-linear projection"

> "The Q and K matrices are then multiplied using a dot-product operation to form the attention matrix QK^T ∈ R^(C×C), which captures relationships between different spatial-temporal segments (in the C dimension)"

> "scaled dot-product attention primarily applies attention across the C dimension, allowing the model to focus on dependencies between spatial-temporal segments across channels and time"

The paper explicitly states that attention operates across the C dimension, where C = M × P (number of variables multiplied by patch size), resulting in a C×C attention matrix.

### Current Implementation

The current implementation in `src/blocks/attention.py` operates differently. In the `SegmentAttentionStage.forward()` method:

```python
def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
    # x shape: [batch, N, C] where N=32, C=112 for ETTh1
    ps_output = self.ps_block(x)  # [batch, N, C]
    
    Q = ps_output
    K = ps_output
    V = ps_output
    
    # Apply attention - this creates [batch, N, N] attention matrix
    attention_output, attention_weights = self.attention(Q, K, V)
```

The `ScaledDotProductAttention.forward()` method processes these tensors as:

```python
# Compute attention scores: Q @ K^T
scores = torch.matmul(Q, K.transpose(-2, -1))  # [batch, num_queries, num_keys]
# This results in [batch, N, N] for input [batch, N, C]
```

### Deviation Impact

**Current Implementation:**
- Input tensor shape: `[batch, N, C]` where N=32 segments, C=112 features
- Attention matrix shape: `[batch, N, N]` 
- Attention focus: Relationships between different temporal segments
- Semantic meaning: "Which time segments should influence each other"

**Paper Specification:**
- Input tensor shape: `[batch, N, C]` (same input format)
- Expected attention matrix shape: `[batch, C, C]`
- Attention focus: Relationships between features within segments
- Semantic meaning: "Which feature positions within segments should influence each other"

## Technical Implications

The current implementation computes attention across the sequence dimension (N), determining which temporal segments attend to which other segments. The paper specification computes attention across the feature dimension (C), determining which feature positions within the concatenated cross-channel segments attend to each other.

This represents a fundamental difference in the attention mechanism's focus. The paper's approach emphasizes spatial-temporal feature relationships within segments, while the current implementation emphasizes temporal relationships between segments.

## Recommendation

The research team should evaluate whether to maintain the current intuitive approach (attending across temporal segments) or align with the paper's specification (attending across spatial-temporal features). Both approaches have theoretical merit, but consistency with the published research may be important for reproducibility and comparison purposes.

If alignment with the paper is desired, the attention computation would need modification to transpose the input tensors and operate across the C dimension rather than the N dimension, while ensuring output shape compatibility with subsequent processing stages.